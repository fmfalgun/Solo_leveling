# Complete and Comprehensive List of Machine Learning, AI, and Deep Learning Algorithms

**Last Updated:** December 2025
**Total Coverage:** 350+ Algorithms & Methods

---

## Table of Contents

1. [Supervised Learning Algorithms](#supervised-learning-algorithms)
2. [Unsupervised Learning Algorithms](#unsupervised-learning-algorithms)
3. [Deep Learning Algorithms](#deep-learning-algorithms)
4. [Reinforcement Learning Algorithms](#reinforcement-learning-algorithms)
5. [Meta-Learning & Few-Shot Learning](#meta-learning--few-shot-learning)
6. [Federated & Distributed Learning](#federated--distributed-learning)
7. [Continual & Incremental Learning](#continual--incremental-learning)
8. [Neural Architecture Search & AutoML](#neural-architecture-search--automl)
9. [Self-Supervised & Contrastive Learning](#self-supervised--contrastive-learning)
10. [Adversarial Training & Robustness](#adversarial-training--robustness)
11. [Online Learning & Bandit Algorithms](#online-learning--bandit-algorithms)
12. [Advanced Graph Learning](#advanced-graph-learning)
13. [Prompt Engineering & In-Context Learning](#prompt-engineering--in-context-learning)
14. [Multimodal Learning](#multimodal-learning)
15. [Mixture of Experts & Sparse Models](#mixture-of-experts--sparse-models)
16. [Neural ODEs & Differential Equations](#neural-odes--differential-equations)
17. [Energy-Based Models](#energy-based-models)
18. [Advanced Optimization Methods](#advanced-optimization-methods)
19. [Advanced Training Techniques](#advanced-training-techniques)
20. [Normalization Methods](#normalization-methods)
21. [Advanced Attention Mechanisms](#advanced-attention-mechanisms)
22. [Knowledge Distillation & Compression](#knowledge-distillation--compression)
23. [Transfer Learning & Domain Adaptation](#transfer-learning--domain-adaptation)
24. [Semi-Supervised & Weak Supervision](#semi-supervised--weak-supervision)
25. [Time Series & Temporal Learning](#time-series--temporal-learning)
26. [Dimensionality Reduction (Extended)](#dimensionality-reduction-extended)

---

## SUPERVISED LEARNING ALGORITHMS

### Regression Algorithms

- Linear Regression
- Logistic Regression
- Ridge Regression (L2 Regularization)
- Lasso Regression (L1 Regularization)
- Elastic Net Regression
- Polynomial Regression
- Support Vector Regression (SVR)
- Decision Tree Regressor
- K-Nearest Neighbors Regressor (KNN Regressor)
- Random Forest Regressor
- Gradient Boosting Regressor
- AdaBoost Regressor
- XGBoost Regressor
- LightGBM Regressor
- CatBoost Regressor
- Neural Network Regressor (MLP Regressor)
- Isotonic Regression
- Theil-Sen Estimator
- HuberRegressor
- TweedieRegressor
- PoissonRegressor
- GammaRegressor

### Classification Algorithms

- Logistic Regression
- Decision Trees
- Random Forest
- Support Vector Machine (SVM)
- Naive Bayes (Gaussian, Multinomial, Bernoulli)
- K-Nearest Neighbors (KNN)
- Gradient Boosting Machines (GBM)
- XGBoost (Extreme Gradient Boosting)
- AdaBoost (Adaptive Boosting)
- LightGBM (Light Gradient Boosting Machine)
- CatBoost (Categorical Boosting)
- Linear Discriminant Analysis (LDA)
- Quadratic Discriminant Analysis (QDA)
- Neural Networks (Multilayer Perceptron - MLP)
- Gaussian Process Classifier
- Extra Trees Classifier (Extremely Randomized Trees)
- Stochastic Gradient Descent (SGD) Classifier
- Passive-Aggressive Classifier
- Ridge Classifier
- One-vs-Rest (OvR) Classifier
- One-vs-One (OvO) Classifier
- Calibrated Classifier CV
- Bagging Classifier
- Voting Classifier
- Multinomial Logistic Regression

### Ensemble Methods (Supervised)

- Bagging
- Bootstrap Aggregating
- Boosting
- Gradient Boosting
- AdaBoost
- Stacking
- Blending
- Voting Classifiers/Regressors
- Cascading Classifiers
- Mixture of Experts (Supervised variant)
- Weighted Averaging Ensemble
- Meta-Learners for Stacking

---

## UNSUPERVISED LEARNING ALGORITHMS

### Clustering Algorithms

- K-Means
- K-Medoids
- Mini Batch K-Means
- K-Means++
- Hierarchical Clustering (Agglomerative & Divisive)
- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
- OPTICS (Ordering Points To Identify the Clustering Structure)
- Gaussian Mixture Models (GMM)
- Fuzzy C-Means
- Mean Shift
- Affinity Propagation
- Spectral Clustering
- BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)
- Self-Organizing Maps (SOM)
- X-Means
- G-Means
- AP Clustering (Affinity Propagation)
- Chameleon Clustering
- CURE (Clustering Using Representatives)
- ROCK (Robust Clustering using linKs)

### Dimensionality Reduction Algorithms

- Principal Component Analysis (PCA)
- Incremental PCA
- Sparse PCA
- Singular Value Decomposition (SVD)
- t-Distributed Stochastic Neighbor Embedding (t-SNE)
- Locally Linear Embedding (LLE)
- Isomap
- Multidimensional Scaling (MDS)
- Independent Component Analysis (ICA)
- Factor Analysis
- Autoencoders
- Variational Autoencoder (VAE)
- Denoising Autoencoder
- Sparse Autoencoder
- Convolutional Autoencoder
- Non-negative Matrix Factorization (NMF)
- Truncated SVD
- Feature Agglomeration
- UMAP (Uniform Manifold Approximation and Projection)
- Manifold Learning
- Kernel PCA
- Random Projection

### Anomaly Detection Algorithms

- Isolation Forest
- Local Outlier Factor (LOF)
- One-Class SVM
- Minimum Covariance Determinant (MCD)
- Robust Covariance
- Gaussian Mixture Models (GMM)
- K-Nearest Neighbors (KNN) for Anomaly Detection
- Mahalanobis Distance
- Z-Score Method
- Modified Z-Score
- Elliptic Envelope
- Empirical Covariance

### Association Rule Learning Algorithms

- Apriori
- Eclat (Equivalence Class Transformation)
- FP-Growth (Frequent Pattern Growth)
- ECLAT
- H-struct
- MaxMiner
- RelIM
- Correlation Rule Learning

### Density Estimation Algorithms

- Kernel Density Estimation (KDE)
- Gaussian Mixture Models (GMM)
- Parzen Windows
- Histogram-based Methods
- Multivariate Kernel Density Estimation

---

## DEEP LEARNING ALGORITHMS

### Feedforward Neural Networks

- Multilayer Perceptron (MLP)
- Deep Neural Networks (DNN)
- Feedforward Neural Network (FFNN)
- Fully Connected Networks (FCN)
- Deep Feedforward Networks with Batch Normalization
- Residual Feedforward Networks

### Convolutional Neural Networks (CNN)

- Basic CNN
- AlexNet
- VGG (Visual Geometry Group) - VGG11, VGG13, VGG16, VGG19
- ResNet (Residual Networks) - ResNet18, ResNet34, ResNet50, ResNet101, ResNet152
- Inception Network (GoogLeNet)
- Inception v2, v3, v4
- MobileNet (MobileNetV1, MobileNetV2, MobileNetV3)
- EfficientNet (EfficientNetB0 through B7)
- SqueezeNet
- DenseNet (DenseNet121, DenseNet169, DenseNet201)
- U-Net
- ResNeXt
- SEResNet (Squeeze-and-Excitation ResNet)
- NASNet
- Xception
- ShuffleNet
- CSPDarknet
- EfficientDet
- YOLOv1/v2/v3/v4/v5/v6/v7/v8
- SSD (Single Shot MultiBox Detector)
- Faster R-CNN
- Mask R-CNN
- RetinaNet
- FCOS (Fully Convolutional One-Stage Object Detection)

### Recurrent Neural Networks (RNN)

- Vanilla RNN
- Long Short-Term Memory (LSTM)
- Gated Recurrent Unit (GRU)
- Bidirectional LSTM (BiLSTM)
- Bidirectional GRU (BiGRU)
- Peephole LSTM
- Clockwork RNN
- Hierarchical RNN
- Tree-Structured LSTM
- Grid LSTM
- Coupled Oscillatory RNN
- Minimal Gated Unit (MGU)

### Attention-Based Models & Transformers

- Transformer
- Self-Attention Mechanism
- Multi-Head Attention
- Cross-Attention
- BERT (Bidirectional Encoder Representations from Transformers)
- BERT variants (RoBERTa, ALBERT, DistilBERT, ELECTRA)
- GPT (Generative Pre-trained Transformer)
- GPT-2, GPT-3, GPT-4
- Vision Transformer (ViT)
- T5 (Text-to-Text Transfer Transformer)
- RoBERTa (Robustly Optimized BERT Pretraining Approach)
- XLNET
- ERNIE
- UniLM (Unified Language Model)
- Longformer
- BigBird
- Performer (FAVOR+ attention)
- Linformer
- Flash Attention
- Linear Attention
- Sparse Attention
- Reformer (Reversible Residual Networks)
- Synthesizer
- Attention is All You Need (Original Transformer Paper Implementation)

### Generative Models

- Generative Adversarial Network (GAN)
- Deep Convolutional GAN (DCGAN)
- Conditional GAN (CGAN)
- Wasserstein GAN (WGAN)
- WGAN-GP (Gradient Penalty)
- StyleGAN
- StyleGAN2
- StyleGAN3
- CycleGAN
- Pix2Pix
- Progressive GAN
- StarGAN
- BigGAN
- Variational Autoencoder (VAE)
- Adversarial Autoencoder (AAE)
- Multi-Adversarial Autoencoder (MAAE)
- PixelCNN
- PixelCNN++
- WaveNet
- NeRF (Neural Radiance Fields)
- SDF-based Generation
- Diffusion Models
- Denoising Diffusion Probabilistic Models (DDPM)
- Score-Based Generative Modeling
- Latent Diffusion Models
- Normalizing Flows
- Flow-based Generative Models (Glow, Flow++)
- Autoregressive Models
- VQ-VAE (Vector Quantized VAE)
- VQ-VAE-2
- VQ-GAN
- Transformer-based Generative Models (GPT variants)

### Restricted/Probabilistic Models

- Restricted Boltzmann Machine (RBM)
- Deep Belief Network (DBN)
- Boltzmann Machine
- Contrastive Divergence Learning
- Belief Propagation Networks
- Markov Random Fields (MRF)
- Conditional Random Fields (CRF)

### Autoencoders and Variants

- Autoencoder
- Denoising Autoencoder
- Sparse Autoencoder
- Variational Autoencoder (VAE)
- Convolutional Autoencoder
- Stacked Autoencoder
- Adversarial Autoencoder (AAE)
- Beta-VAE
- Disentangled VAE
- Wasserstein Autoencoder (WAE)
- Vector Quantized VAE (VQ-VAE)
- VQ-VAE-2
- VQ-GAN
- Contractive Autoencoder
- Concrete Autoencoder

### Sequence-to-Sequence Models

- Encoder-Decoder Architecture
- Sequence-to-Sequence (Seq2Seq) with Attention
- Neural Machine Translation (NMT)
- Attention Mechanism in Seq2Seq
- Multi-head Attention Seq2Seq
- ULMFiT (Universal Language Model Fine-Tuning)
- Transformer Seq2Seq
- Pointer Networks
- Hierarchical Attention Networks

### Graph Neural Networks (GNNs)

- Graph Convolutional Network (GCN)
- Graph Attention Network (GAT)
- Graph Attention Network v2 (GATv2)
- Graph Isomorphism Network (GIN)
- GraphSAGE (Graph SAmple and aggreGatE)
- Message Passing Neural Network (MPNN)
- Spectral Graph Networks
- Graph Residual Networks
- GraphRNN
- Heterogeneous Graph Neural Networks
- Knowledge Graph Embeddings (TransE, TransH, TransR)
- Graph Autoencoders
- Variational Graph Autoencoders (VGAE)
- Deep Graph Infomax (DGI)
- GraphCL (Graph Contrastive Learning)
- Temporal Graph Networks
- Dynamic Graph Neural Networks
- Spatial-Temporal Graph Networks

### Temporal/Time Series Models

- Temporal Convolutional Network (TCN)
- WaveNet
- Temporal Fusion Transformer
- N-BEATS (Neural Basis Expansion Analysis with Time Series)
- DeepAR (Autoregressive Recurrent Networks for Time Series)
- Transformer-based Time Series (Informer, Autoformer, FEDformer)
- Temporal Point Process Networks
- Neural ODE for Time Series
- Deep State Space Models
- Latent ODEs

---

## REINFORCEMENT LEARNING ALGORITHMS

### Value-Based Methods

- Q-Learning
- Deep Q-Networks (DQN)
- Double DQN (DDQN)
- Dueling DQN
- Prioritized Experience Replay (PER)
- Rainbow DQN
- Expected Sarsa
- Sarsa (State-Action-Reward-State-Action)
- Sarsa-Lambda
- Temporal Difference (TD) Learning
- TD(λ) Learning
- Value Iteration
- Asynchronous Methods for Deep RL

### Policy-Based Methods

- REINFORCE (Policy Gradient)
- Policy Gradient
- Deterministic Policy Gradient (DPG)
- Trust Region Policy Optimization (TRPO)
- Proximal Policy Optimization (PPO)
- Deep Deterministic Policy Gradient (DDPG)
- Twin Delayed DDPG (TD3)
- Policy Improvement with Path Integrals (PI²)
- Natural Gradient Policy Search
- Actor-Critic (AC)
- Advantage Actor-Critic (A2C)
- Asynchronous Advantage Actor-Critic (A3C)
- Soft Actor-Critic (SAC)
- Distributional RL (C51, QR-DQN, IQN)
- Implicit Quantile Networks (IQN)

### Model-Based Methods

- Dyna-Q
- World Models
- Model-Predictive Control (MPC)
- Imagination-Augmented Agents (I2A)
- PILCO (Probabilistic Inference for Learning Control)
- PETS (Probabilistic Ensembles with Trajectory Sampling)
- Dreamer (Dream to Control: Learning Behaviors by Latent Imagination)
- PlaNet (Learning Latent Dynamics for Action Repeat in Visual RL)
- Latent-Space Models

### Multi-Agent Reinforcement Learning

- QMIX (Mixing Networks for Multi-Agent Q-Learning)
- MARL (Multi-Agent Reinforcement Learning)
- MADDPG (Multi-Agent DDPG)
- MAPPO (Multi-Agent PPO)
- COMA (Counterfactual Multi-Agent Policy Gradients)
- QPLEX (Factorizing Value Function for Multi-Agent RL)
- Centralized Training with Decentralized Execution (CTDE)
- Independent Learners
- Communication-based Multi-Agent RL

### Inverse Reinforcement Learning

- Maximum Entropy IRL
- Apprenticeship Learning
- Guided Cost Learning
- Bayesian IRL
- Maximum Causal Entropy IRL
- Generative Adversarial Imitation Learning (GAIL)

### Hierarchical Reinforcement Learning

- Options Framework
- Feudal Networks
- HAMs (Hierarchical Abstract Machines)
- MAXQ Value Function Decomposition
- Temporal Abstraction
- Hierarchical Deep RL

### Exploration Methods & Curiosity

- Epsilon-Greedy
- Upper Confidence Bound (UCB)
- Thompson Sampling
- Curiosity-Driven Learning
- Intrinsic Motivation
- Random Network Distillation (RND)
- Empowerment-based Exploration
- Count-based Exploration
- Prediction Error as Intrinsic Reward

---

## META-LEARNING & FEW-SHOT LEARNING

### Core Meta-Learning

- MAML (Model-Agnostic Meta-Learning)
- Prototypical Networks
- Matching Networks
- Relation Networks
- Meta-Transfer Learning (MTL)
- Meta-SGD
- Reptile
- CAML (Context Adaptation for Meta-Learning)
- ADML (Adversarial Meta-Learner)
- Task-Adaptive Meta-Learning (TAML)
- LearnToLearn Framework
- First-Order MAML (FOMAML)
- Probabilistic Model-Agnostic Meta-Learning (PMAML)

### Few-Shot Learning Specific

- Siamese Networks
- Matching Networks
- Relation Networks
- Meta-Transfer Learning (MTL)
- Adaptive Meta-Transfer (A-MET)
- Meta-Transfer Adjustment (MTA)
- Prototypical Networks
- Matching Networks
- Transfer Learning for Few-Shot

### Zero-Shot Learning

- Zero-Shot Classification
- Semantic Attribute-Based Zero-Shot Learning
- Embedding-Based Zero-Shot Learning
- Generative Zero-Shot Learning
- Transductive Zero-Shot Learning

### Multi-Task Learning

- Multi-Task Learning (MTL)
- Hard Parameter Sharing
- Soft Parameter Sharing
- Task-Specific Attention
- Cross-Stitch Networks
- Multi-Task Self-Attention Networks
- MTL with Uncertainty Weighting

---

## FEDERATED & DISTRIBUTED LEARNING

### Core Federated Learning

- Federated Averaging (FedAvg)
- Federated SGD (FedSGD)
- FedProx
- FedMA (Federated Learning with Matched Averaging)
- FedBN (Federated Learning with Batch Normalization)
- Sub-FedAvg
- HeteroFL (Heterogeneous Federated Learning)
- FedOpt (Federated Learning with Server and Client Optimization)

### Advanced Federated Methods

- SCAFFOLD (Stochastic Controlled Averaging for Federated Learning)
- FedAdam (Federated Optimization with Adam)
- FedYogi
- MOON (Model-Contrastive Federated Learning)
- FedDyn (Federated Dynamic Regularization)
- Personalized Federated Learning (Per-FedAvg)
- FedPAQ (Federated Learning with Adaptive Quantization)
- FedDyn
- Federated Proximal Methods

### Federated with AutoML

- AutoFL
- FLASH (Federated Learning with Automated Search)
- HANF (Hyperparameter and Architecture for Neural Federated)
- Federated NAS
- Federated AutoML

### Distributed Learning (Non-Federated)

- Distributed SGD
- Distributed Training
- Parameter Server Architecture
- All-Reduce Pattern
- Pipeline Parallelism
- Data Parallelism
- Model Parallelism
- Ring All-Reduce

---

## CONTINUAL & INCREMENTAL LEARNING

### Continual Learning Methods

- Elastic Weight Consolidation (EWC)
- Learning without Forgetting (LwF)
- iCaRL (Incremental Classifier and Representation Learning)
- Progressive Neural Networks
- PackNet
- Gradient Episodic Memory (GEM)
- Averaged Gradient Episodic Memory (A-GEM)
- Experience Replay (ER)
- AGEM (A-GEM with Exemplar Selection)
- Memory Replay Networks
- Continual World Models
- Dark Experience for General Continual Learning (DER)

### Incremental Learning

- Learn++
- IDE4, ID5R
- Incremental SVM
- Fuzzy ARTMAP
- TopoART
- IGNG (Incremental Growing Neural Gas)
- Gaenari
- Incremental Decision Trees

### Task/Domain/Class Incremental Learning

- Task-Incremental Learning
- Domain-Incremental Learning
- Class-Incremental Learning
- Online Class-Incremental Continual Learning
- Domain Adaptation Incremental Learning

---

## NEURAL ARCHITECTURE SEARCH & AUTOML

### NAS Methods

- NASNet
- ENAS (Efficient Neural Architecture Search)
- DARTS (Differentiable Architecture Search)
- PNAS (Progressive Neural Architecture Search)
- AmoebaNet
- ProxylessNAS
- FBNet
- MnasNet
- EfficientNets (via NAS)
- AutoKeras
- GDAS (Gradient-based Differentiable Architecture Search)
- CDARTS (Cyclic Differentiable Architecture Search)
- PC-DARTS (Partial Channel Connections)
- RandWireNN (Random Wired Neural Networks)
- Graph-based NAS
- One-Shot NAS
- Weight-Sharing NAS

### Hyperparameter Optimization

- BOHB (Bayesian Optimization and HyperBand)
- Hyperband
- ASHA (Asynchronous Successive Halving Algorithm)
- SMAC (Sequential Model-based Algorithm Configuration)
- CASH (Combined Algorithm Selection and Hyperparameter optimization)
- TPE (Tree-structured Parzen Estimator)
- Random Search
- Grid Search
- Bayesian Optimization
- Gaussian Process Optimization
- Particle Swarm Optimization
- Differential Evolution
- Simulated Annealing

### AutoML Systems

- Auto-sklearn
- H2O AutoML
- TPOT (Tree-based Pipeline Optimization Tool)
- Google AutoML
- AutoGluon
- PyCaret
- AutoKeras
- MLBox
- Auto-WEKA
- Hyperopt
- Optuna
- Ray Tune

---

## SELF-SUPERVISED & CONTRASTIVE LEARNING

### Contrastive Learning

- SimCLR (Simple Framework for Contrastive Learning)
- SimCLR v2
- MoCo (Momentum Contrast)
- MoCo v2
- MoCo v3
- BYOL (Bootstrap Your Own Latent)
- SwAV (Swapping Assignments between Views)
- SimSiam
- Barlow Twins
- VICReg (Variance-Invariance-Covariance Regularization)
- DINO (Self-Distillation with No Labels)
- DINOv2
- iBOT (Image BERT Pre-Training with Online Tokenizer)
- Masked Feature Prediction
- MAE (Masked Autoencoder)

### Other Self-Supervised Methods

- AMDIM (Augmented Multiscale Deep InfoMax)
- CPC (Contrastive Predictive Coding)
- CPC v2
- NNCLR (Nearest-Neighbor Contrastive Learning)
- PIRL (Pretext-Invariant Representation Learning)
- InfoMin
- PixelPro
- DeepCluster
- SwAV (Swapped Assignment with mulTiview transformation)
- MoCo-v3
- Contrastive Representation Learning
- Supervised Contrastive Learning (SupCon)
- Triplet Loss-based Learning

### Masked Image Modeling

- SimMIM / PixMIM (Masked-image modeling / pixel reconstruction)
- BEiT (BERT Pre-Training of Image Transformers)
- MAE (Masked Autoencoders Are Scalable Vision Learners)
- CAE (Context Autoencoder)
- W-MAE (Weakly-Supervised MAE)

---

## ADVERSARIAL TRAINING & ROBUSTNESS

### Adversarial Attack Methods

- FGSM (Fast Gradient Sign Method)
- IFGSM (Iterative FGSM)
- PGD (Projected Gradient Descent)
- C&W Attack (Carlini & Wagner)
- DeepFool
- JSMA (Jacobian-based Saliency Map Attack)
- Universal Adversarial Perturbations (UAP)
- One-Pixel Attack
- Boundary Attack
- HopSkipJumpAttack
- Square Attack
- AutoAttack
- Sparse Adversarial Attack
- Semantic Adversarial Examples
- Physical Adversarial Examples

### Adversarial Defense Methods

- Adversarial Training
- PGD Adversarial Training
- FGSM Adversarial Training
- Fast Adversarial Training
- Free Adversarial Training
- TRADES (TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization)
- Defensive Distillation
- Adversarial Logit Pairing
- Certified Robustness
- Randomized Smoothing
- Certified Adversarial Robustness
- Robust Feature Representations
- Adversarial Purification
- Thermometer Encoding

---

## ONLINE LEARNING & BANDIT ALGORITHMS

### Core Online Learning

- Online Gradient Descent (OGD)
- Follow-the-Leader
- Follow-the-Regularized-Leader (FTRL)
- Online Newton Step
- Adaptive Gradient Descent (AdaGrad variants for online)
- Online Passive-Aggressive Algorithms
- Perceptron (Online version)
- Winnow
- Online SVM
- Online Decision Trees

### Bandit Algorithms

- Multi-Armed Bandit (MAB)
- Upper Confidence Bound (UCB)
- UCB1, UCB2, UCB-V
- Thompson Sampling
- LinUCB (Linear Upper Confidence Bound)
- Contextual Bandits
- Contextual Thompson Sampling
- Exp3 (Exponential-weight algorithm for Exploration and Exploitation)
- Exp3.P (Exp3 with Exploration)
- BayesUCB
- Bayesian Bandits
- Combinatorial Bandits
- Dueling Bandits

### Regret Bounds & Analysis

- Adversarial Bandits
- Stochastic Bandits
- Best-Arm Identification
- Pure Exploration in Bandits

---

## ADVANCED GRAPH LEARNING

### Core Graph Methods

- Graph Attention Networks v2 (GATv2)
- Spectral Graph Networks
- Deep Graph Infomax (DGI)
- GraphCL (Graph Contrastive Learning)
- Heterogeneous Graph Neural Networks
- Temporal Graph Networks
- Dynamic Graph Neural Networks
- Graph Isomorphism Network (GIN)
- Graph Residual Networks
- GraphRNN
- Knowledge Graph Embeddings (TransE, TransH, TransR, RotatE)
- Graph Autoencoders
- Variational Graph Autoencoders (VGAE)
- Spatial-Temporal Graph Networks

### Knowledge Graphs

- Knowledge Graph Completion
- Link Prediction in Knowledge Graphs
- Entity Alignment
- Relation Extraction
- Graph Embeddings
- Node2Vec
- GraphSAGE
- DeepWalk

### Graph Generation

- Graph Generative Models
- GraphRNN
- GRAN (Graph Recurrent Attention Networks)
- Molecular Graph Generation
- Diffusion Models for Graphs

---

## PROMPT ENGINEERING & IN-CONTEXT LEARNING

### Prompt Engineering Techniques

- Prompt Tuning
- Prefix Tuning
- P-Tuning
- Chain-of-Thought Prompting
- Tree-of-Thought (ToT)
- ReAct (Reasoning + Acting)
- Few-Shot Prompting
- Zero-Shot Prompting
- In-Context Learning (ICL)
- Instruction Tuning

### Large Language Model (LLM) Adaptation

- LoRA (Low-Rank Adaptation)
- QLoRA (Quantized Low-Rank Adaptation)
- Adapter Modules
- BitFit (Bias-term Fine-tuning)
- Soft Prompting
- Hard Prompting
- Demonstration Learning
- Meta-Prompting

---

## MULTIMODAL LEARNING

### Cross-Modal Methods

- CLIP (Contrastive Language-Image Pre-training)
- ALIGN (Scaling Up Visual and Vision-Language Joint Input Representations)
- DALL-E (Text-to-Image Generation)
- DALL-E 2
- DALL-E 3
- Flamingo (A Visual Language Model for Few-Shot Learning)
- BLIP (Bootstrapping Language-Image Pre-training)
- Perceiver (General Perception with Iterative Attention)
- Perceiver IO
- ViLBERT (Vision-and-Language BERT)
- LXMERT (Learning Cross-Modality Encoder Representations from Transformers)
- UNITER (Universal Image-Text Representation Learning)
- ViLT (Vision-and-Language Transformer)
- ALBEF (Align Before Fusing)
- OFA (Unifying Architectures, Tasks, and Modalities)
- Multimodal Fusion Techniques

---

## MIXTURE OF EXPERTS & SPARSE MODELS

### Mixture of Experts

- Mixture of Experts (MoE)
- Sparse Mixture of Experts
- Dense Mixture of Experts
- Switch Transformer
- GLaM (Generalist Language Model)
- Sparse Gating MoE
- Expert Choice Routing
- Top-K Gating
- Base Layer Routing
- Shared Expert Models
- Task-Specific Experts

---

## NEURAL ODES & DIFFERENTIAL EQUATIONS

### Neural ODE Methods

- Neural Ordinary Differential Equations (Neural ODE)
- Augmented Neural ODEs
- Continuous Neural Processes
- Neural Process Variants
- Latent ODEs
- Adversarial Neural ODEs

### Physics-Informed Methods

- Hamiltonian Neural Networks
- Symplectic ODE-Net
- Physics-Informed Neural Networks (PINNs)
- Lagrangian Neural Networks
- Energy-Conserving Neural Networks

---

## ENERGY-BASED MODELS

- Energy-Based GANs
- Contrastive Divergence
- Joint Energy-Based Models
- Score Matching
- Noise Contrastive Estimation (NCE)
- Denoising Score Matching
- Sliced Score Matching
- Score-Based Generative Models
- Diffusion Models (Score-Based)

---

## ADVANCED OPTIMIZATION METHODS

### Modern Optimizers

- Adam
- AdamW
- RAdam (Rectified Adam)
- AdaBound
- AdamGrad
- Nadam
- AdaBelief
- Lion Optimizer (EvoLved Sign Momentum)
- Sophia Optimizer
- Lookahead Optimizer
- LAMB (Layer-wise Adaptive Moments for Batch training)
- Shampoo
- K-FAC (Kronecker-Factored Approximate Curvature)
- Adagrad
- Adadelta
- RMSprop
- SGD with Momentum
- Nesterov Momentum
- Proximal Methods
- Mirror Descent

### Learning Rate Scheduling

- Constant Learning Rate
- Step Decay
- Exponential Decay
- Polynomial Decay
- Cosine Annealing
- Warm Restarts (SGDR)
- Cyclical Learning Rate
- OneCycleLR
- ReduceLROnPlateau
- Learning Rate Finder

---

## ADVANCED TRAINING TECHNIQUES

### Data Augmentation

- Mixup
- CutMix
- Cutout
- RandAugment
- AutoAugment
- Manifold Mixup
- MixUp Variants
- Augmentation via GANs
- Neural Style Transfer for Augmentation
- Mosaic Augmentation

### Regularization Techniques

- Dropout
- DropConnect
- DropBlock
- Stochastic Depth
- Zoneout
- Batch Normalization
- Layer Normalization
- Instance Normalization
- Group Normalization
- L1/L2 Regularization
- Elastic Net Penalty
- Early Stopping
- Weight Decay
- Label Smoothing
- Mixup as Regularization
- Shake-Shake Regularization

### Advanced Training

- Curriculum Learning
- Hard Example Mining
- Focal Loss
- Class Imbalance Handling
- Contrastive Learning during Training
- Multi-scale Training
- Progressive Training
- Knowledge Distillation during Training
- Feature Alignment

---

## NORMALIZATION METHODS

- Batch Normalization (BatchNorm)
- Layer Normalization (LayerNorm)
- Group Normalization (GroupNorm)
- Instance Normalization (InstanceNorm)
- Weight Normalization
- Spectral Normalization
- LayerScale
- RMSNorm
- FilterResponseNorm (FRN)
- LocalResponseNorm
- Batch Renormalization
- Switchable Normalization
- EvoNorm (Evolving Normalization-Activation Layers)

---

## ADVANCED ATTENTION MECHANISMS

### Attention Variants

- Self-Attention
- Multi-Head Attention
- Cross-Attention
- Flash Attention
- Linear Attention
- Sparse Attention
- Longformer Attention
- BigBird Attention
- Performer (FAVOR+ attention)
- Linformer
- Reformer (Local + Global Attention)
- Clustered Attention
- Synthesizer Attention
- Sparse Transformers
- Axial Attention

---

## KNOWLEDGE DISTILLATION & COMPRESSION

### Knowledge Distillation

- Knowledge Distillation (Standard)
- Self-Distillation
- Dark Knowledge
- Teacher-Student Learning
- Noisy Student
- Born-Again Networks
- Attention Transfer
- Mutual Learning
- Feature-Based Distillation
- Relation-Based Distillation
- Multi-Teacher Distillation
- Cross-Modal Distillation

### Neural Compression

- Pruning (Magnitude-based, Lottery Ticket Hypothesis)
- Structured Pruning
- Unstructured Pruning
- Quantization (Post-Training Quantization)
- Quantization-Aware Training (QAT)
- Mixed-Precision Quantization
- Binary Networks
- Ternary Networks
- Low-Rank Factorization
- Matrix Factorization
- Tensor Decomposition
- LoRA (Low-Rank Adaptation)
- QLoRA (Quantized Low-Rank Adaptation)
- Knowledge Distillation for Compression
- Neural Architecture for Efficiency

---

## TRANSFER LEARNING & DOMAIN ADAPTATION

### Transfer Learning

- Pre-trained Models
- Fine-tuning Approaches
- Feature Extraction
- Shallow Transfer Learning
- Deep Transfer Learning
- Multitask Transfer Learning
- Domain Generalization
- Source-Free Domain Adaptation

### Domain Adaptation

- Unsupervised Domain Adaptation
- Semi-Supervised Domain Adaptation
- Open-Set Domain Adaptation
- Partial Domain Adaptation
- Multi-Source Domain Adaptation
- Domain-Adversarial Training
- Maximum Mean Discrepancy (MMD)
- CORAL (Correlation Alignment)
- Self-Training for Domain Adaptation
- Progressive Domain Adaptation
- Batch Normalization Adaptation

---

## SEMI-SUPERVISED & WEAK SUPERVISION

### Semi-Supervised Learning

- Self-Training
- Co-Training
- Tri-Training
- Label Propagation
- Graph-Based Semi-Supervised Learning
- Pseudo-Labeling
- Consistency Regularization (MixMatch, FixMatch, ReMixMatch)
- MixMatch
- FixMatch
- ReMixMatch
- UDA (Unsupervised Data Augmentation)
- Virtual Adversarial Training
- Mean Teacher
- Temporal Ensembling
- Semi-Supervised VAE

### Weak Supervision

- Noisy Labels Learning
- Label Noise Robust Learning
- Learning with Crowds
- Active Learning
- Weakly Supervised Learning
- Coupled Pattern Learner (CPL)
- Distant Supervision
- Crowdsourcing for Labels
- Multiple Instance Learning (MIL)

---

## TIME SERIES & TEMPORAL LEARNING

### Time Series Methods

- Temporal Convolutional Network (TCN)
- WaveNet
- Temporal Fusion Transformer
- N-BEATS (Neural Basis Expansion Analysis with Time Series)
- DeepAR (Autoregressive Recurrent Networks for Time Series)
- Transformer-based Time Series
- Informer (Attention is All You Need for Time Series)
- Autoformer (Decomposition Transformers for Forecasting)
- FEDformer (Fourier Enhanced Decomposed Transformers)
- Temporal Point Process Networks
- Neural ODE for Time Series
- Deep State Space Models
- Seq2Seq for Time Series
- LSTM for Time Series
- GRU for Time Series
- Attention Mechanisms for Time Series
- Multi-Scale Temporal Convolutional Networks
- Variational Inference for Time Series
- Bayesian Time Series Models
- Gaussian Process for Time Series
- ARIMA (AutoRegressive Integrated Moving Average)
- SARIMA (Seasonal ARIMA)
- Vector AutoRegression (VAR)
- Prophet (Time Series Forecasting)

---

## DIMENSIONALITY REDUCTION (EXTENDED)

### Advanced Dimensionality Reduction

- Principal Component Analysis (PCA)
- Incremental PCA
- Sparse PCA
- Probabilistic PCA
- Singular Value Decomposition (SVD)
- Truncated SVD
- t-Distributed Stochastic Neighbor Embedding (t-SNE)
- Locally Linear Embedding (LLE)
- Isomap
- Multidimensional Scaling (MDS)
- Independent Component Analysis (ICA)
- Factor Analysis
- Kernel PCA
- Random Projection
- Feature Agglomeration
- UMAP (Uniform Manifold Approximation and Projection)
- Manifold Learning (General)
- Laplacian Eigenmaps
- Hessian Eigenmaps
- Local Tangent Space Alignment (LTSA)
- Diffusion Maps
- Autoencoder-based Dimensionality Reduction
- Variational Autoencoder (VAE) for Dimensionality Reduction
- Non-negative Matrix Factorization (NMF)
- Sparse Coding
- Dictionary Learning
- Latent Dirichlet Allocation (LDA) (for text)
- Latent Semantic Analysis (LSA)
- Partial Least Squares (PLS)

---

## ADDITIONAL IMPORTANT ALGORITHMS & PARADIGMS

### Natural Language Processing Specific

- Tokenization (BPE, WordPiece, SentencePiece)
- Word Embeddings (Word2Vec, GloVe, FastText)
- Subword Embeddings
- Contextualized Embeddings (ELMo, BERT)
- Named Entity Recognition (NER)
- Dependency Parsing
- Semantic Role Labeling
- Question Answering
- Machine Translation
- Summarization
- Text Classification
- Sentiment Analysis
- Relation Extraction

### Computer Vision Specific

- Object Detection (YOLO, Faster R-CNN, SSD, EfficientDet)
- Semantic Segmentation
- Instance Segmentation
- Panoptic Segmentation
- Pose Estimation
- Face Recognition
- Face Detection
- Action Recognition
- Video Classification
- 3D Object Detection
- 3D Reconstruction
- Structure from Motion
- Optical Flow
- Depth Estimation

### Speech & Audio Processing

- Speech Recognition (ASR)
- Text-to-Speech (TTS)
- Voice Conversion
- Speaker Recognition
- Speaker Diarization
- Music Generation
- Audio Classification
- Source Separation
- Voice Activity Detection

### Bayesian Methods

- Bayesian Networks
- Hidden Markov Models (HMM)
- Markov Chain Monte Carlo (MCMC)
- Variational Inference
- Gibbs Sampling
- Metropolis-Hastings Algorithm
- Hamiltonian MCMC
- No-U-Turn Sampler (NUTS)
- Probabilistic Programming
- Bayesian Optimization
- Gaussian Processes

### Explainability & Interpretability

- LIME (Local Interpretable Model-Agnostic Explanations)
- SHAP (SHapley Additive exPlanations)
- Attention Visualization
- Saliency Maps
- Integrated Gradients
- DeepLIFT
- Layer-wise Relevance Propagation (LRP)
- Feature Importance
- Permutation Importance
- Partial Dependence Plots
- TCAV (Testing with Concept Activation Vectors)
- Influence Functions
- Example-based Explanations

---

## QUICK REFERENCE BY LEARNING TYPE

### Supervised Learning
**Count: 90+ algorithms**
- Regression (22+), Classification (23+), Ensemble Methods (6+)

### Unsupervised Learning
**Count: 60+ algorithms**
- Clustering (20+), Dimensionality Reduction (22+), Anomaly Detection (12+), Association Learning (4+), Density Estimation (5+)

### Deep Learning
**Count: 100+ architectures/algorithms**
- CNNs (30+), RNNs/LSTMs/GRUs (10+), Transformers (15+), GANs (20+), Autoencoders (13+), Graph Networks (15+), Temporal Models (15+)

### Reinforcement Learning
**Count: 50+ algorithms**
- Value-Based (12+), Policy-Based (12+), Actor-Critic (10+), Model-Based (7+), Multi-Agent (5+), Hierarchical (5+), Exploration (5+), IRL (3+)

### Meta/Few-Shot Learning
**Count: 20+ algorithms**

### Federated & Distributed Learning
**Count: 20+ algorithms**

### Continual & Incremental Learning
**Count: 20+ algorithms**

### NAS & AutoML
**Count: 35+ methods**

### Self-Supervised & Contrastive Learning
**Count: 30+ algorithms**

### Adversarial & Robustness
**Count: 25+ methods**

### Online & Bandit Learning
**Count: 25+ algorithms**

### Advanced Methods (Optimization, Attention, Compression, etc.)
**Count: 80+ methods**

**Total Comprehensive Coverage: 350+ Machine Learning, AI, and Deep Learning Algorithms**

---

## IMPLEMENTATION LIBRARIES & FRAMEWORKS

### Machine Learning
- **scikit-learn**: Most supervised and unsupervised algorithms
- **XGBoost**: Extreme Gradient Boosting
- **LightGBM**: Light Gradient Boosting Machine
- **CatBoost**: Categorical Boosting
- **Statsmodels**: Statistical modeling

### Deep Learning
- **TensorFlow/Keras**: Comprehensive deep learning framework
- **PyTorch**: Research-friendly deep learning framework
- **JAX**: Composable transformations of NumPy programs

### Reinforcement Learning
- **OpenAI Gym**: RL environments
- **Stable Baselines3**: RL algorithm implementations
- **RLlib**: Distributed RL
- **DM Control**: DeepMind control suite

### Specialized Libraries
- **Hugging Face Transformers**: Pre-trained models and NLP
- **AllenNLP**: Deep learning NLP library
- **OpenCV**: Computer vision
- **Librosa**: Audio processing
- **Gensim**: Topic modeling
- **NetworkX**: Graph algorithms
- **PyG (PyTorch Geometric)**: Graph neural networks
- **DGL (Deep Graph Library)**: Graph neural networks
- **Ray Tune**: Hyperparameter optimization
- **Optuna**: Hyperparameter optimization
- **AutoGluon**: AutoML
- **NNI (Neural Network Intelligence)**: AutoML and NAS

---

## NOTES & EMERGING TRENDS

### 1. Emerging Importance (2024-2025)
- **Federated Learning**: Critical for privacy-preserving AI and distributed systems
- **Continual Learning**: Essential for lifelong learning systems
- **Self-Supervised Learning**: Revolutionized representation learning with minimal labels
- **Diffusion Models**: Becoming dominant generative model paradigm
- **Efficient Models**: LoRA, QLoRA, Mixture of Experts for scalable deployment

### 2. Recent Breakthroughs
- **DINO, SimCLR, MoCo, BYOL**: Transformed self-supervised learning in computer vision
- **Vision Transformers (ViT)**: Challenged CNN dominance in vision tasks
- **Multimodal Models (CLIP, DALL-E)**: Unified vision and language understanding
- **Diffusion Models**: Achieved state-of-the-art generation quality
- **Large Language Models**: GPT-3/4 demonstrated emergent abilities at scale

### 3. AutoML & NAS Revolution
- Neural Architecture Search democratized access to high-performance models
- Automated hyperparameter optimization now essential for practical ML deployment
- End-to-end AutoML systems simplify model development

### 4. Robustness & Safety Focus
- Adversarial training and certified robustness critical for safety-critical applications
- Verification and interpretability gaining prominence
- Edge case handling and OOD detection increasingly important

### 5. Efficiency Trends
- **LoRA/QLoRA**: Enable efficient fine-tuning of large models
- **Mixture of Experts**: Conditional computation for scalability
- **Neural Compression**: Pruning, quantization for deployment
- **Mobile & Edge ML**: TinyML for resource-constrained devices

### 6. Foundation Model Era (2024-2025)
- **Prompt Engineering**: Fundamental for working with large language models
- **In-Context Learning**: Few-shot adaptation without fine-tuning
- **Instruction Tuning**: Teaching models to follow instructions
- **Adapter Modules**: Efficient model specialization

### 7. Paradigm Shifts
- **From Supervised to Self-Supervised**: Reduced reliance on expensive labeled data
- **From Static to Continual**: Systems adapted to lifelong learning
- **From Centralized to Federated**: Privacy-preserving collaborative learning
- **From Task-Specific to Foundation**: General-purpose models adapted to tasks

### 8. Integration Considerations
- Modern applications combine multiple algorithms (CNN-LSTM for video, Transformer-CNN for vision-language)
- Ensemble methods provide robustness and improved performance
- Hybrid supervised-unsupervised approaches leverage available data
- Theory + practice balance essential for production systems

### 9. Future Directions
- Neuromorphic computing and spiking neural networks
- Quantum machine learning algorithms
- Causal inference and causal machine learning
- Neuro-symbolic AI combining neural and symbolic approaches
- More efficient and sustainable AI methods

---

## SUMMARY STATISTICS

| Category | Count |
|----------|-------|
| Supervised Learning | 90+ |
| Unsupervised Learning | 60+ |
| Deep Learning | 100+ |
| Reinforcement Learning | 50+ |
| Meta/Few-Shot Learning | 20+ |
| Federated & Distributed | 20+ |
| Continual Learning | 20+ |
| NAS & AutoML | 35+ |
| Self-Supervised & Contrastive | 30+ |
| Adversarial & Robustness | 25+ |
| Online & Bandit Learning | 25+ |
| Advanced Optimization & Training | 80+ |
| NLP/CV/Audio Specific | 60+ |
| Bayesian & Probabilistic | 15+ |
| Explainability & Interpretability | 15+ |
| **TOTAL** | **350+** |

---

## ACKNOWLEDGMENTS & REFERENCES

This comprehensive compilation combines algorithms and methods from:
- Recent academic research (2020-2025)
- Industry practitioners and open-source communities
- Leading ML frameworks and libraries
- Conference proceedings (NeurIPS, ICML, ICCV, EMNLP, etc.)
- ArXiv papers and technical blogs

For the most up-to-date information and implementation details, refer to:
- Official framework documentation (TensorFlow, PyTorch)
- Hugging Face Model Hub
- Papers with Code
- ArXiv.org
- Conference websites and proceedings
